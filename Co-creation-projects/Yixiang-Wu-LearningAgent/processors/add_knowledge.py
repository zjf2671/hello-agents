# processors/add_knowledge.py
"""çŸ¥è¯†æ·»åŠ å¤„ç†å™¨ - ä½¿ç”¨ LLM åˆ†æã€åˆ†ç±»å¹¶ä¿å­˜çŸ¥è¯†"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from hello_agents import HelloAgentsLLM
from core.file_manager import FileManager
from core.summary_manager import SummaryManager


class AddKnowledgeProcessor:
    """
    çŸ¥è¯†æ·»åŠ å¤„ç†å™¨

    åŠŸèƒ½ï¼š
    - è¯†åˆ«è¾“å…¥ç±»å‹ï¼ˆæ–‡æœ¬/æ–‡ä»¶/URLï¼‰
    - ä½¿ç”¨ LLM åˆ†æå†…å®¹
    - æ™ºèƒ½åˆ†ç±»å’Œæ‰“æ ‡ç­¾
    - æå–å…³é”®æ¦‚å¿µ
    - ç”Ÿæˆæ–‡ä»¶å
    - ä¿å­˜åˆ° knowledge ç›®å½•
    - æ›´æ–° knowledge_summary.md
    """

    def __init__(self, llm: HelloAgentsLLM, file_manager: FileManager):
        """
        åˆå§‹åŒ– AddKnowledgeProcessor

        Args:
            llm: HelloAgentsLLM å®ä¾‹
            file_manager: FileManager å®ä¾‹
        """
        self.llm = llm
        self.file_manager = file_manager
        self.summary_manager = SummaryManager(file_manager)

    def _identify_input_type(self, input_data: str) -> str:
        """
        è¯†åˆ«è¾“å…¥ç±»å‹

        Args:
            input_data: ç”¨æˆ·è¾“å…¥

        Returns:
            è¾“å…¥ç±»å‹ï¼ˆtext/file/urlï¼‰
        """
        # æ£€æŸ¥ URL
        if input_data.startswith("http://") or input_data.startswith("https://"):
            return "url"

        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
        if (
            input_data.startswith("~")
            or input_data.startswith("/")
            or input_data.startswith("./")
        ):
            return "file"

        # é»˜è®¤ä¸ºæ–‡æœ¬
        return "text"

    def _read_file(self, file_path: str) -> str:
        """
        è¯»å–æ–‡ä»¶å†…å®¹

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ–‡ä»¶å†…å®¹
        """
        # å¤„ç† ~ è·¯å¾„
        if file_path.startswith("~"):
            file_path = os.path.expanduser(file_path)

        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()

    def _analyze_content(self, content: str, domain: str) -> Dict[str, any]:
        """
        ä½¿ç”¨ LLM åˆ†æå†…å®¹

        Args:
            content: çŸ¥è¯†å†…å®¹
            domain: é¢†åŸŸåç§°

        Returns:
            åˆ†æç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - category: åˆ†ç±»
            - tags: æ ‡ç­¾åˆ—è¡¨
            - key_concepts: å…³é”®æ¦‚å¿µåˆ—è¡¨
            - summary: æ‘˜è¦
        """
        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹çŸ¥è¯†å†…å®¹å¹¶æå–å…³é”®ä¿¡æ¯ï¼š

ã€é¢†åŸŸã€‘
{domain}

ã€çŸ¥è¯†å†…å®¹ã€‘
{content[:2000]}

è¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼‰ï¼š
{{
  "category": "åˆ†ç±»ï¼ˆå¦‚ï¼šç®—æ³•ã€æ¦‚å¿µã€å·¥å…·ã€å®è·µç­‰ï¼‰",
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"],
  "key_concepts": ["æ ¸å¿ƒæ¦‚å¿µ1", "æ ¸å¿ƒæ¦‚å¿µ2", "æ ¸å¿ƒæ¦‚å¿µ3"],
  "summary": "ä¸€å¥è¯æ‘˜è¦ï¼ˆ50å­—ä»¥å†…ï¼‰"
}}
"""

        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†ç®¡ç†ä¸“å®¶ï¼Œæ“…é•¿åˆ†æå­¦ä¹ å†…å®¹å¹¶æå–å…³é”®ä¿¡æ¯ã€åˆ†ç±»å’Œæ ‡ç­¾ã€‚",
            },
            {"role": "user", "content": user_prompt},
        ]

        try:
            response = self.llm.invoke(messages)

            # å°è¯•è§£æ JSONï¼ˆç®€åŒ–å®ç°ï¼šä½¿ç”¨è§„åˆ™æå–ï¼‰
            return self._extract_metadata_from_text(response)
        except Exception:
            # é™çº§ï¼šä½¿ç”¨è§„åˆ™åˆ†æ
            return {
                "category": self._classify_content(content, domain),
                "tags": self._extract_tags_from_content(content),
                "key_concepts": self._extract_concepts_from_content(content),
                "summary": content[:100] + "..." if len(content) > 100 else content,
                "domain": domain,  # æ·»åŠ  domain å­—æ®µ
            }

    def _extract_metadata_from_text(self, text: str) -> Dict[str, any]:
        """
        ä»æ–‡æœ¬ä¸­æå–å…ƒæ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼‰

        Args:
            text: LLM å“åº”æ–‡æœ¬

        Returns:
            å…ƒæ•°æ®å­—å…¸
        """
        # ç®€åŒ–å®ç°ï¼šåŸºäºè§„åˆ™æå–
        lines = text.strip().split("\n")

        category = "é€šç”¨"
        tags = []
        key_concepts = []
        summary = ""

        for line in lines:
            line = line.strip()
            if "åˆ†ç±»" in line or "category" in line.lower():
                category = line.split("ï¼š")[-1].split(":")[-1].strip()
            elif "æ ‡ç­¾" in line or "tags" in line.lower():
                tags = [
                    tag.strip(" \"'[]{}")
                    for tag in line.split("ï¼š")[-1].split(":")[-1].split(",")
                ]
            elif "æ¦‚å¿µ" in line or "concepts" in line.lower():
                key_concepts = [
                    c.strip(" \"'[]{}")
                    for c in line.split("ï¼š")[-1].split(":")[-1].split(",")
                ]
            elif "æ‘˜è¦" in line or "summary" in line.lower():
                summary = line.split("ï¼š")[-1].split(":")[-1].strip()

        return {
            "category": category if category else "é€šç”¨",
            "tags": [t for t in tags if t],
            "key_concepts": [c for c in key_concepts if c],
            "summary": summary if summary else "çŸ¥è¯†ç¬”è®°",
            "domain": domain,  # æ·»åŠ  domain å­—æ®µ
        }

    def _extract_tags_from_content(self, content: str) -> List[str]:
        """
        ä»å†…å®¹ä¸­æå–æ ‡ç­¾ï¼ˆåŸºäºå…³é”®è¯ï¼‰

        Args:
            content: å†…å®¹æ–‡æœ¬

        Returns:
            æ ‡ç­¾åˆ—è¡¨
        """
        # å¸¸è§æŠ€æœ¯å…³é”®è¯
        keywords = [
            "ç®—æ³•",
            "æ•°æ®ç»“æ„",
            "æœºå™¨å­¦ä¹ ",
            "æ·±åº¦å­¦ä¹ ",
            "Python",
            "JavaScript",
            "TypeScript",
            "Java",
            "æ¡†æ¶",
            "åº“",
            "å·¥å…·",
            "API",
            "å‰ç«¯",
            "åç«¯",
            "å…¨æ ˆ",
            "æ•°æ®åº“",
            "ç†è®º",
            "å®è·µ",
            "æ•™ç¨‹",
            "ç¤ºä¾‹",
        ]

        found = []
        content_lower = content.lower()
        for keyword in keywords:
            if keyword.lower() in content_lower:
                found.append(keyword)

        return found[:5]  # æœ€å¤š5ä¸ªæ ‡ç­¾

    def _extract_concepts_from_content(self, content: str) -> List[str]:
        """
        ä»å†…å®¹ä¸­æå–å…³é”®æ¦‚å¿µ

        Args:
            content: å†…å®¹æ–‡æœ¬

        Returns:
            å…³é”®æ¦‚å¿µåˆ—è¡¨
        """
        # æå–ä»¥ # å¼€å¤´çš„æ ‡é¢˜ä½œä¸ºæ¦‚å¿µ
        concepts = []
        for line in content.split("\n"):
            line = line.strip()
            if line.startswith("#"):
                # å»æ‰ # ç¬¦å·å’Œç©ºæ ¼
                concept = line.lstrip("#").strip()
                if concept and len(concept) < 50:  # é™åˆ¶é•¿åº¦
                    concepts.append(concept)

        return concepts[:5]  # æœ€å¤š5ä¸ªæ¦‚å¿µ

    def _generate_filename(self, title: str, category: str = "") -> str:
        """
        ç”Ÿæˆæ–‡ä»¶å

        Args:
            title: æ ‡é¢˜
            category: åˆ†ç±»ï¼ˆå¯é€‰ï¼‰

        Returns:
            æ–‡ä»¶åï¼ˆå¸¦æ‰©å±•åï¼‰
        """
        # æå–ç¬¬ä¸€å¥è¯ä½œä¸ºæ–‡ä»¶å
        if len(title) > 50:
            title = title[:50]

        # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        title = title.replace(" ", "-")
        title = "".join(c for c in title if c.isalnum() or c in "-_")

        # æ·»åŠ æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d-%H%M")

        if category:
            base_name = f"{timestamp}-{category}-{title}"
        else:
            base_name = f"{timestamp}-{title}"

        return f"{base_name}.md"  # æ·»åŠ  .md æ‰©å±•å

    def _save_knowledge(
        self, domain: str, content: str, metadata: Dict[str, any]
    ) -> Path:
        """
        ä¿å­˜çŸ¥è¯†ç¬”è®°

        Args:
            domain: é¢†åŸŸåç§°
            content: çŸ¥è¯†å†…å®¹
            metadata: å…ƒæ•°æ®

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆ_generate_filename å·²åŒ…å« .md æ‰©å±•åï¼‰
        title = content.split("\n")[0].lstrip("#").strip()
        filename = self._generate_filename(title, metadata.get("category", ""))

        # æ·»åŠ å…ƒæ•°æ®åˆ°å†…å®¹
        full_content = f"""# {title}

> **åˆ†ç±»**: {metadata.get('category', 'é€šç”¨')}
> **æ ‡ç­¾**: {', '.join(metadata.get('tags', []))}
> **æ·»åŠ æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}

---

{content}

## å…³é”®æ¦‚å¿µ
{chr(10).join(f"- {c}" for c in metadata.get('key_concepts', []))}

## æ‘˜è¦
{metadata.get('summary', 'æ— ')}
"""

        # ä¿å­˜æ–‡ä»¶
        self.file_manager.save_knowledge(domain, filename, full_content)

        # è¿”å›å®Œæ•´è·¯å¾„
        return self.file_manager.BASE_DIR / domain / "knowledge" / filename

    def _classify_content(self, content: str, domain: str) -> str:
        """
        åˆ†ç±»å†…å®¹

        Args:
            content: å†…å®¹
            domain: é¢†åŸŸ

        Returns:
            åˆ†ç±»åç§°
        """
        # åŸºäºè§„åˆ™çš„ç®€å•åˆ†ç±»
        content_lower = content.lower()

        if any(
            word in content_lower for word in ["ç®—æ³•", "algorithm", "æ–¹æ³•", "method"]
        ):
            return "ç®—æ³•"
        elif any(
            word in content_lower for word in ["æ¦‚å¿µ", "concept", "åŸç†", "principle"]
        ):
            return "æ¦‚å¿µ"
        elif any(
            word in content_lower
            for word in ["å·¥å…·", "tool", "æ¡†æ¶", "framework", "åº“", "library"]
        ):
            return "å·¥å…·"
        elif any(
            word in content_lower
            for word in ["å®è·µ", "practice", "æ¡ˆä¾‹", "case", "é¡¹ç›®", "project"]
        ):
            return "å®è·µ"
        elif any(
            word in content_lower for word in ["æ•™ç¨‹", "tutorial", "æŒ‡å—", "guide"]
        ):
            return "æ•™ç¨‹"
        else:
            return "é€šç”¨"

    def add(self, domain: str, input_data: str, input_type: str = None) -> str:
        """
        æ·»åŠ çŸ¥è¯†

        Args:
            domain: é¢†åŸŸåç§°
            input_data: è¾“å…¥æ•°æ®ï¼ˆæ–‡æœ¬/æ–‡ä»¶è·¯å¾„/URLï¼‰
            input_type: è¾“å…¥ç±»å‹ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨è¯†åˆ«ï¼‰

        Returns:
            æ‰§è¡Œç»“æœ
        """
        # è¯†åˆ«è¾“å…¥ç±»å‹
        if not input_type:
            input_type = self._identify_input_type(input_data)

        # è·å–å†…å®¹
        if input_type == "text":
            content = input_data
        elif input_type == "file":
            try:
                content = self._read_file(input_data)
            except Exception as e:
                return f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ï¼š{e}"
        elif input_type == "url":
            # ç®€åŒ–å®ç°ï¼šæç¤ºç”¨æˆ·å¤åˆ¶å†…å®¹
            content = f"# URL çŸ¥è¯†\n\næ¥æºï¼š{input_data}\n\nè¯·æ‰‹åŠ¨æ·»åŠ å†…å®¹..."
        else:
            return f"âŒ æœªçŸ¥çš„è¾“å…¥ç±»å‹ï¼š{input_type}"

        # åˆ†æå†…å®¹
        metadata = self._analyze_content(content, domain)

        # ä¿å­˜çŸ¥è¯†
        try:
            file_path = self._save_knowledge(domain, content, metadata)

            # æ›´æ–°æ‘˜è¦
            self.summary_manager.update_knowledge_summary(domain, file_path.name)

            return f"""âœ… çŸ¥è¯†å·²æ·»åŠ 

ğŸ“ ä¿å­˜ä½ç½®: {domain}/knowledge/{file_path.name}
ğŸ“Š åˆ†ç±»: {metadata.get('category', 'é€šç”¨')}
ğŸ·ï¸  æ ‡ç­¾: {', '.join(metadata.get('tags', []))}
"""

        except Exception as e:
            return f"âŒ æ·»åŠ çŸ¥è¯†å¤±è´¥ï¼š{e}"
