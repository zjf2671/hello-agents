"""
YouTube è§†é¢‘æœç´¢è„šæœ¬ - æŒ‰ä¸»é¢˜æœç´¢ã€è¯„åˆ†ã€ç”Ÿæˆæ—¥æŠ¥
ä» themes.yaml è¯»å–ä¸»é¢˜åˆ—è¡¨ï¼Œå¯¹æ¯ä¸ªä¸»é¢˜åˆ†åˆ«æœç´¢ YouTube
åˆå¹¶ç»“æœã€è¯„åˆ†ã€æ’åºåç”Ÿæˆæ—¥æŠ¥æŠ¥å‘Š
"""

import sys
import os
import json
import argparse
import re
from pathlib import Path
from datetime import datetime, timedelta, timezone

# è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸ºUTF-8ï¼ˆWindowsï¼‰
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

try:
    import httpx
except ImportError:
    print("âŒ é”™è¯¯: éœ€è¦å®‰è£… httpx åº“")
    print("ğŸ’¡ è¿è¡Œ: pip install httpx")
    sys.exit(1)

try:
    import yaml
except ImportError:
    print("âŒ é”™è¯¯: éœ€è¦å®‰è£… PyYAML åº“")
    print("ğŸ’¡ è¿è¡Œ: pip install pyyaml")
    sys.exit(1)

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # dotenv å¯é€‰ï¼Œå¦‚æœæœªå®‰è£…åˆ™è·³è¿‡

# å¯é€‰ï¼šå¯¼å…¥ LLM ç›¸å…³æ¨¡å—ï¼ˆä»…ç”¨äº research æ¨¡å¼ï¼‰
try:
    from hello_agents.core.llm import HelloAgentsLLM
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False

# é…ç½®å¸¸é‡
DAYS_WINDOW = int(os.getenv("DAYS_WINDOW", "14"))  # æ—¶é—´çª—å£ï¼šé»˜è®¤14å¤©


def load_youtube_api_key():
    """ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­åŠ è½½ YouTube API Key"""
    # é¦–å…ˆå°è¯•ç¯å¢ƒå˜é‡
    api_key = os.getenv("YOUTUBE_API_KEY")
    
    if api_key:
        return api_key
    
    # å°è¯•ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–
    config_file = Path(__file__).parent / "config"
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith("YOUTUBE_API_KEY=") and not line.startswith("#"):
                        api_key = line.split("=", 1)[1].strip()
                        if api_key:
                            return api_key
        except Exception as e:
            print(f"âš ï¸  è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    return None


def load_themes():
    """ä» themes.yaml è¯»å–ä¸»é¢˜åˆ—è¡¨"""
    themes_file = Path(__file__).parent / "themes.yaml"
    if not themes_file.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° themes.yaml æ–‡ä»¶: {themes_file}")
        return []
    
    try:
        with open(themes_file, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
            if data is None:
                print(f"âŒ é”™è¯¯: themes.yaml æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
                return []
            themes = data.get('themes', [])
            if not themes:
                print(f"âš ï¸  è­¦å‘Š: themes.yaml ä¸­æœªæ‰¾åˆ°ä¸»é¢˜åˆ—è¡¨")
                return []
            print(f"âœ… åŠ è½½äº† {len(themes)} ä¸ªä¸»é¢˜: {', '.join(themes)}")
            return themes
    except Exception as e:
        print(f"âŒ è¯»å– themes.yaml å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


def load_whitelist_channels():
    """ä» channels.yaml è¯»å–ç™½åå•é¢‘é“"""
    channels_file = Path(__file__).parent / "channels.yaml"
    if not channels_file.exists():
        print(f"âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ° channels.yaml æ–‡ä»¶: {channels_file}")
        return []
    
    try:
        with open(channels_file, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
            if data is None:
                print(f"âš ï¸  è­¦å‘Š: channels.yaml æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
                return []
            channels = data.get('whitelist_channels', [])
            print(f"âœ… åŠ è½½äº† {len(channels)} ä¸ªç™½åå•é¢‘é“")
            return channels
    except Exception as e:
        print(f"âš ï¸  è¯»å– channels.yaml å¤±è´¥: {e}")
        return []


def search_youtube_videos(query: str, max_results: int = 10, api_key: str = None):
    """æœç´¢ YouTube è§†é¢‘"""
    if not api_key:
        api_key = load_youtube_api_key()
    
    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° YouTube API Key")
        print("ğŸ’¡ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ YOUTUBE_API_KEY æˆ–åœ¨ config æ–‡ä»¶ä¸­é…ç½®")
        return None
    
    try:
        url = "https://www.googleapis.com/youtube/v3/search"
        params = {
            "key": api_key,
            "q": query,
            "part": "snippet",
            "type": "video",
            "maxResults": min(max_results, 50),  # API limit
            "order": "relevance"
        }
        
        response = httpx.get(url, params=params, timeout=10.0)
        response.raise_for_status()
        
        data = response.json()
        
        if "items" not in data or not data["items"]:
            return []
        
        videos = []
        for item in data["items"]:
            video_info = {
                "video_id": item["id"]["videoId"],
                "title": item["snippet"]["title"],
                "description": item["snippet"]["description"],
                "channel_title": item["snippet"]["channelTitle"],
                "channel_id": item["snippet"]["channelId"],
                "published_at": item["snippet"]["publishedAt"],
                "thumbnail": item["snippet"]["thumbnails"].get("medium", {}).get("url", ""),
                "url": f"https://www.youtube.com/watch?v={item['id']['videoId']}",
                "query": query  # è®°å½•æœç´¢å…³é”®è¯
            }
            videos.append(video_info)
        
        return videos
    
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 403:
            print(f"âŒ é”™è¯¯: API å¯†é’¥æ— æ•ˆæˆ–é…é¢å·²ç”¨å®Œ (æŸ¥è¯¢: {query})")
        else:
            print(f"âŒ HTTP é”™è¯¯: {e.response.status_code} (æŸ¥è¯¢: {query})")
        return None
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥ (æŸ¥è¯¢: {query}): {str(e)}")
        return None


def parse_published_time(published_at_str: str):
    """è§£æå‘å¸ƒæ—¶é—´å­—ç¬¦ä¸²ä¸º datetime å¯¹è±¡"""
    try:
        # YouTube API è¿”å› ISO 8601 æ ¼å¼: 2024-01-01T12:00:00Z
        dt = datetime.fromisoformat(published_at_str.replace('Z', '+00:00'))
        return dt
    except Exception as e:
        print(f"âš ï¸  è§£æå‘å¸ƒæ—¶é—´å¤±è´¥: {published_at_str}, é”™è¯¯: {e}")
        return None


def is_within_time_window(published_at_str: str, days_window: int = DAYS_WINDOW):
    """æ£€æŸ¥è§†é¢‘æ˜¯å¦åœ¨æ—¶é—´çª—å£å†…ï¼ˆé»˜è®¤14å¤©ï¼‰"""
    published_time = parse_published_time(published_at_str)
    if not published_time:
        return False
    
    now = datetime.now(timezone.utc)
    time_diff = now - published_time
    
    return time_diff <= timedelta(days=days_window)


def calculate_time_score(published_at_str: str):
    """è®¡ç®—æ—¶é—´è¯„åˆ†ï¼š24å°æ—¶å†… +3ï¼Œ48å°æ—¶å†… +2"""
    published_time = parse_published_time(published_at_str)
    if not published_time:
        return 0
    
    now = datetime.now(timezone.utc)
    time_diff = now - published_time
    
    if time_diff <= timedelta(hours=24):
        return 3
    elif time_diff <= timedelta(hours=48):
        return 2
    else:
        return 0


def count_theme_keywords(text: str, themes: list):
    """è®¡ç®—æ–‡æœ¬ä¸­å‘½ä¸­çš„ä¸»é¢˜å…³é”®è¯æ•°é‡ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰"""
    if not text:
        return 0
    
    text_lower = text.lower()
    count = 0
    for theme in themes:
        if theme.lower() in text_lower:
            count += 1
    return count


def score_video(video: dict, themes: list, whitelist_channels: list):
    """ä¸ºè§†é¢‘è®¡ç®—è¯„åˆ†"""
    score = 0
    
    # 1. ç™½åå•é¢‘é“è¯„åˆ† +10
    if video['channel_title'] in whitelist_channels:
        score += 10
    
    # 2. æ ‡é¢˜æˆ–æè¿°ä¸­æ¯å‘½ä¸­1ä¸ªä¸»é¢˜å…³é”®è¯ +5
    title_matches = count_theme_keywords(video['title'], themes)
    desc_matches = count_theme_keywords(video['description'], themes)
    keyword_score = (title_matches + desc_matches) * 5
    score += keyword_score
    
    # 3. å‘å¸ƒæ—¶é—´è¯„åˆ†
    time_score = calculate_time_score(video['published_at'])
    score += time_score
    
    return score


def merge_and_deduplicate_videos(all_videos: list):
    """åˆå¹¶è§†é¢‘åˆ—è¡¨å¹¶æŒ‰ videoId å»é‡"""
    video_dict = {}
    
    for video in all_videos:
        video_id = video['video_id']
        if video_id not in video_dict:
            video_dict[video_id] = video
        else:
            # å¦‚æœå·²å­˜åœ¨ï¼Œåˆå¹¶æŸ¥è¯¢å…³é”®è¯
            existing_queries = video_dict[video_id].get('queries', [])
            if isinstance(existing_queries, str):
                existing_queries = [existing_queries]
            if video['query'] not in existing_queries:
                existing_queries.append(video['query'])
            video_dict[video_id]['queries'] = existing_queries
    
    return list(video_dict.values())


def generate_action(videos: list):
    """ç”Ÿæˆ action å­—æ®µï¼šä» Top1 ç”Ÿæˆ1æ¡å¯æ‰§è¡ŒåŠ¨ä½œï¼ˆâ‰¤15minï¼‰"""
    if not videos:
        return "æš‚æ— æ¨èè§†é¢‘"
    
    # åªä½¿ç”¨ Top1
    top1 = videos[0]
    action = f"è§‚çœ‹ã€Š{top1['title']}ã€‹({top1['channel_title']})ï¼Œé¢„è®¡â‰¤15åˆ†é’Ÿ"
    
    return action


def has_clickbait_words(title: str):
    """æ£€æŸ¥æ ‡é¢˜ä¸­æ˜¯å¦åŒ…å«æ ‡é¢˜å…šè¯æ±‡"""
    clickbait_words = ['INSANE', 'HYPE', 'SHOCKING', 'UNBELIEVABLE', 'MIND-BLOWING', 
                       'AMAZING', 'INCREDIBLE', 'YOU WON\'T BELIEVE', 'THIS WILL BLOW YOUR MIND']
    title_upper = title.upper()
    for word in clickbait_words:
        if word in title_upper:
            return True
    return False


def is_older_than_days(published_at_str: str, days: int = 30):
    """æ£€æŸ¥è§†é¢‘æ˜¯å¦è¶…è¿‡æŒ‡å®šå¤©æ•°"""
    published_time = parse_published_time(published_at_str)
    if not published_time:
        return False
    
    now = datetime.now(timezone.utc)
    time_diff = now - published_time
    
    return time_diff > timedelta(days=days)


def generate_risk(videos: list, themes: list):
    """ç”Ÿæˆ risk å­—æ®µï¼šåå·®æ£€æµ‹"""
    if not videos:
        return "æ— é£é™©"
    
    # åªæ£€æŸ¥ Top3
    top3 = videos[:3]
    warnings = []
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶…è¿‡30å¤©çš„è§†é¢‘
    old_videos = []
    for video in top3:
        if is_older_than_days(video['published_at'], days=30):
            old_videos.append(video['title'])
    
    if old_videos:
        warnings.append(f"Top3ä¸­å­˜åœ¨è¶…è¿‡30å¤©çš„è§†é¢‘: {', '.join(old_videos[:2])}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡é¢˜å…šè¯æ±‡
    clickbait_videos = []
    for video in top3:
        if has_clickbait_words(video['title']):
            clickbait_videos.append(video['title'])
    
    if clickbait_videos:
        warnings.append(f"æ£€æµ‹åˆ°æ ‡é¢˜å…šè¯æ±‡: {', '.join(clickbait_videos[:2])}")
    
    # å¦‚æœæœ‰è­¦å‘Šï¼Œè¿”å›è­¦å‘Šï¼›å¦åˆ™è¿”å›æ­£é¢è¯„ä»·
    if warnings:
        return "; ".join(warnings)
    else:
        return "ä»Šæ—¥ä¿¡å·è¾ƒæ–°ä¸”è¾ƒå¯ä¿¡"


def init_research_llm():
    """åˆå§‹åŒ–ç”¨äºç ”ç©¶æ¨¡å¼çš„ LLMï¼ˆä½¿ç”¨é€šä¹‰åƒé—®/ModelScopeé…ç½®ï¼‰"""
    if not LLM_AVAILABLE:
        print("âš ï¸  è­¦å‘Š: hello_agents æ¨¡å—æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ç ”ç©¶æ¨¡å¼")
        return None
    
    # ä»ç¯å¢ƒå˜é‡è¯»å– LLM é…ç½®ï¼ˆä¼˜å…ˆçº§é¡ºåºï¼Œä¸ chapter9 ä¿æŒä¸€è‡´ï¼‰
    # ä¼˜å…ˆä½¿ç”¨ ModelScope é…ç½®ï¼ˆé€šä¹‰åƒé—®ï¼‰
    llm_model = (
        os.getenv("LLM_MODEL") or 
        os.getenv("LLM_MODEL_ID") or
        "Qwen/Qwen2.5-7B-Instruct"  # é»˜è®¤é€šä¹‰åƒé—®æ¨¡å‹
    )
    llm_api_key = (
        os.getenv("LLM_API_KEY") or  # ä¼˜å…ˆä½¿ç”¨ LLM_API_KEYï¼ˆé˜¿é‡Œäº‘é€šä¹‰åƒé—®ï¼‰
        os.getenv("MODELSCOPE_API_KEY") or 
        os.getenv("MODELSCOPE_API_TOKEN")
    )
    llm_base_url = (
        os.getenv("LLM_BASE_URL") or 
        "https://api-inference.modelscope.cn/v1/"  # ModelScope é»˜è®¤åœ°å€
    )
    llm_provider = os.getenv("LLM_PROVIDER", "modelscope")
    
    if not llm_api_key:
        print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° LLM API Keyï¼Œç ”ç©¶æ¨¡å¼éœ€è¦é…ç½® LLM")
        print("ğŸ’¡ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæ¨èåœ¨ .env æ–‡ä»¶ä¸­é…ç½®ï¼‰:")
        print("   MODELSCOPE_API_KEY=your-modelscope-token-here")
        print("   LLM_MODEL=Qwen/Qwen2.5-7B-Instruct")
        print("   LLM_BASE_URL=https://api-inference.modelscope.cn/v1/")
        print("   LLM_PROVIDER=modelscope")
        return None
    
    try:
        llm = HelloAgentsLLM(
            model=llm_model,
            api_key=llm_api_key,
            base_url=llm_base_url,
            provider=llm_provider
        )
        print(f"âœ… LLM åˆå§‹åŒ–æˆåŠŸ: {llm_model} ({llm_provider})")
        return llm
    except Exception as e:
        print(f"âš ï¸  åˆå§‹åŒ– LLM å¤±è´¥: {e}")
        return None


def prepare_sources_data(top3_videos: list):
    """ä» Top3 è§†é¢‘ä¸­æå– sources æ•°æ®"""
    sources = []
    for video in top3_videos:
        sources.append({
            "title": video['title'],
            "channel": video['channel_title'],
            "url": video['url'],
            "published_at": video['published_at'],
            "score": video['score']
        })
    return sources


def extract_json_from_text(text: str):
    """ä»æ–‡æœ¬ä¸­æå– JSON å†…å®¹ï¼ˆå¤„ç† LLM å¯èƒ½è¿”å›çš„æ ¼å¼åŒ–æ–‡æœ¬ï¼‰"""
    # å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(text.strip())
    except json.JSONDecodeError:
        pass
    
    # å°è¯•æå– JSON ä»£ç å—
    json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # å°è¯•æå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡
    json_match = re.search(r'\{.*\}', text, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(0))
        except json.JSONDecodeError:
            pass
    
    return None


def generate_research_report(top3_videos: list, themes: list, llm):
    """ä½¿ç”¨ LLM ç”Ÿæˆç ”ç©¶æŠ¥å‘Š"""
    if not top3_videos:
        return None
    
    # æ„å»ºè§†é¢‘ä¿¡æ¯æ–‡æœ¬
    videos_info = []
    for i, video in enumerate(top3_videos, 1):
        videos_info.append(
            f"{i}. æ ‡é¢˜: {video['title']}\n"
            f"   é¢‘é“: {video['channel_title']}\n"
            f"   å‘å¸ƒæ—¶é—´: {video['published_at']}\n"
            f"   è¯„åˆ†: {video['score']}åˆ†\n"
            f"   é“¾æ¥: {video['url']}"
        )
    
    videos_text = "\n\n".join(videos_info)
    themes_text = ", ".join(themes)
    
    # æ„å»º prompt
    prompt = f"""åŸºäºä»¥ä¸‹ Top3 YouTube è§†é¢‘ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–ç ”ç©¶æŠ¥å‘Šã€‚

è§†é¢‘ä¿¡æ¯ï¼š
{videos_text}

æœç´¢ä¸»é¢˜ï¼š{themes_text}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ä»¥ä¸‹å†…å®¹ï¼š
1. question: ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œæ¦‚æ‹¬è¿™äº›è§†é¢‘çš„å…±åŒå…³æ³¨ç‚¹
2. key_findings: 3æ¡å‘ç°ï¼Œæ¯æ¡1å¥è¯ï¼ŒåŸºäºæ ‡é¢˜/é¢‘é“/å‘å¸ƒæ—¶é—´æ¨æ–­ï¼Œä½¿ç”¨"å¯èƒ½/å€¾å‘"ç­‰æªè¾
3. why_it_matters_to_me: ä¸ºä»€ä¹ˆè¿™äº›ä¿¡æ¯å¯¹æˆ‘é‡è¦ï¼ˆä¸ªæ€§åŒ–è§£é‡Šï¼‰
4. next_steps: 1-3æ¡è¡ŒåŠ¨å»ºè®®ï¼Œæ¯æ¡â‰¤15åˆ†é’Ÿ

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼ˆä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
  "question": "æ ¸å¿ƒé—®é¢˜",
  "key_findings": [
    "å‘ç°1ï¼ˆä½¿ç”¨å¯èƒ½/å€¾å‘ç­‰æªè¾ï¼‰",
    "å‘ç°2ï¼ˆä½¿ç”¨å¯èƒ½/å€¾å‘ç­‰æªè¾ï¼‰",
    "å‘ç°3ï¼ˆä½¿ç”¨å¯èƒ½/å€¾å‘ç­‰æªè¾ï¼‰"
  ],
  "why_it_matters_to_me": "ä¸ªæ€§åŒ–è§£é‡Š",
  "next_steps": [
    "è¡ŒåŠ¨å»ºè®®1ï¼ˆâ‰¤15åˆ†é’Ÿï¼‰",
    "è¡ŒåŠ¨å»ºè®®2ï¼ˆâ‰¤15åˆ†é’Ÿï¼‰",
    "è¡ŒåŠ¨å»ºè®®3ï¼ˆâ‰¤15åˆ†é’Ÿï¼‰"
  ]
}}"""

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶åˆ†æå¸ˆï¼Œæ“…é•¿ä»è§†é¢‘ä¿¡æ¯ä¸­æå–å…³é”®æ´å¯Ÿå¹¶ç»™å‡ºå¯æ‰§è¡Œçš„è¡ŒåŠ¨å»ºè®®ã€‚è¯·å§‹ç»ˆä»¥ JSON æ ¼å¼è¿”å›ç»“æœã€‚"},
        {"role": "user", "content": prompt}
    ]
    
    try:
        print("\nğŸ”¬ æ­£åœ¨ä½¿ç”¨ LLM ç”Ÿæˆç ”ç©¶æŠ¥å‘Š...")
        response = llm.invoke(messages)
        
        if not response:
            print("âš ï¸  LLM è¿”å›ç©ºå“åº”")
            return None
        
        # æå– JSON
        research_data = extract_json_from_text(response)
        
        if not research_data:
            print(f"âš ï¸  æ— æ³•è§£æ LLM å“åº”ä¸º JSONï¼ŒåŸå§‹å“åº”: {response[:200]}...")
            return None
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ["question", "key_findings", "why_it_matters_to_me", "next_steps"]
        missing_fields = [field for field in required_fields if field not in research_data]
        if missing_fields:
            print(f"âš ï¸  LLM å“åº”ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}")
            return None
        
        # ç¡®ä¿ key_findings æ˜¯åˆ—è¡¨ä¸”æœ‰3æ¡
        if not isinstance(research_data.get("key_findings"), list):
            research_data["key_findings"] = []
        if len(research_data["key_findings"]) != 3:
            # å¦‚æœä¸è¶³3æ¡ï¼Œå¡«å……æˆ–æˆªæ–­
            while len(research_data["key_findings"]) < 3:
                research_data["key_findings"].append("æš‚æ— å‘ç°")
            research_data["key_findings"] = research_data["key_findings"][:3]
        
        # ç¡®ä¿ next_steps æ˜¯åˆ—è¡¨ï¼Œæœ€å¤š3æ¡
        if not isinstance(research_data.get("next_steps"), list):
            research_data["next_steps"] = []
        research_data["next_steps"] = research_data["next_steps"][:3]
        
        print("âœ… ç ”ç©¶æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
        return research_data
        
    except Exception as e:
        print(f"âš ï¸  ç”Ÿæˆç ”ç©¶æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="YouTube è§†é¢‘æœç´¢ - å¤šä¸»é¢˜æ™ºèƒ½æœç´¢ä¸æ—¥æŠ¥ç”Ÿæˆ")
    parser.add_argument(
        "--mode",
        type=str,
        choices=["daily_signal", "research"],
        default="research",
        help="è¿è¡Œæ¨¡å¼: research (é»˜è®¤ï¼Œç”Ÿæˆæ—¥æŠ¥+ç ”ç©¶æŠ¥å‘Š) æˆ– daily_signal (ä»…ç”Ÿæˆæ—¥æŠ¥)"
    )
    args = parser.parse_args()
    mode = args.mode
    
    print("=" * 70)
    print("YouTube è§†é¢‘æœç´¢ - å¤šä¸»é¢˜æ™ºèƒ½æœç´¢ä¸æ—¥æŠ¥ç”Ÿæˆ")
    if mode == "research":
        print("è¿è¡Œæ¨¡å¼: ç ”ç©¶æ¨¡å¼ (å°†ç”Ÿæˆæ—¥æŠ¥ + ç ”ç©¶æŠ¥å‘Š)")
    else:
        print("è¿è¡Œæ¨¡å¼: æ—¥æŠ¥æ¨¡å¼ (ä»…ç”Ÿæˆæ—¥æŠ¥)")
    print("=" * 70)
    
    # 1. åŠ è½½é…ç½®
    themes = load_themes()
    if not themes:
        print("âŒ æ— æ³•åŠ è½½ä¸»é¢˜åˆ—è¡¨ï¼Œé€€å‡º")
        return
    
    whitelist_channels = load_whitelist_channels()
    api_key = load_youtube_api_key()
    if not api_key:
        print("âŒ æ— æ³•åŠ è½½ API Keyï¼Œé€€å‡º")
        return
    
    # 2. å¯¹æ¯ä¸ªä¸»é¢˜æœç´¢
    print(f"\nğŸ” å¼€å§‹æœç´¢ {len(themes)} ä¸ªä¸»é¢˜...")
    all_videos = []
    
    for theme in themes:
        print(f"  æœç´¢ä¸»é¢˜: {theme}")
        videos = search_youtube_videos(theme, max_results=10, api_key=api_key)
        if videos:
            all_videos.extend(videos)
            print(f"    âœ… æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")
        else:
            print(f"    âš ï¸  æœªæ‰¾åˆ°è§†é¢‘æˆ–æœç´¢å¤±è´¥")
    
    if not all_videos:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘ï¼Œé€€å‡º")
        return
    
    print(f"\nğŸ“Š åˆå¹¶å‰å…±æ‰¾åˆ° {len(all_videos)} ä¸ªè§†é¢‘")
    
    # 3. åˆå¹¶å»é‡
    unique_videos = merge_and_deduplicate_videos(all_videos)
    print(f"ğŸ“Š å»é‡åå‰©ä½™ {len(unique_videos)} ä¸ªå”¯ä¸€è§†é¢‘")
    
    # 4. æ—¶é—´çª—å£è¿‡æ»¤ï¼šåªè€ƒè™‘æœ€è¿‘ DAYS_WINDOW å¤©çš„è§†é¢‘
    print(f"\nâ° åº”ç”¨æ—¶é—´çª—å£è¿‡æ»¤ï¼ˆ{DAYS_WINDOW}å¤©ï¼‰...")
    filtered_videos = [v for v in unique_videos if is_within_time_window(v['published_at'], DAYS_WINDOW)]
    excluded_count = len(unique_videos) - len(filtered_videos)
    if excluded_count > 0:
        print(f"   âš ï¸  è¿‡æ»¤æ‰ {excluded_count} ä¸ªè¶…è¿‡ {DAYS_WINDOW} å¤©çš„è§†é¢‘")
    print(f"   âœ… å‰©ä½™ {len(filtered_videos)} ä¸ªè§†é¢‘å‚ä¸æ’åº")
    
    if not filtered_videos:
        print(f"âŒ æ—¶é—´çª—å£å†…ï¼ˆ{DAYS_WINDOW}å¤©ï¼‰æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘ï¼Œé€€å‡º")
        return
    
    # 5. è¯„åˆ†
    print(f"\nâ­ å¼€å§‹è¯„åˆ†...")
    for video in filtered_videos:
        score = score_video(video, themes, whitelist_channels)
        video['score'] = score
        video['scoring_details'] = {
            'whitelist_bonus': 10 if video['channel_title'] in whitelist_channels else 0,
            'keyword_matches': count_theme_keywords(video['title'], themes) + count_theme_keywords(video['description'], themes),
            'time_bonus': calculate_time_score(video['published_at'])
        }
    
    # 6. æ’åºå¹¶å– Top 3
    sorted_videos = sorted(filtered_videos, key=lambda x: x['score'], reverse=True)
    top3_videos = sorted_videos[:3]
    
    print(f"\nğŸ† Top 3 è§†é¢‘:")
    for i, video in enumerate(top3_videos, 1):
        print(f"  {i}. [{video['score']}åˆ†] {video['title']}")
        print(f"     é¢‘é“: {video['channel_title']}")
        print(f"     é“¾æ¥: {video['url']}")
    
    # 7. ç”Ÿæˆæ—¥æœŸå­—ç¬¦ä¸²
    today = datetime.now().strftime("%Y-%m-%d")
    
    # 8. åˆ›å»ºè¾“å‡ºç›®å½•
    base_dir = Path(__file__).parent
    raw_dir = base_dir / "raw" / "youtube"
    archive_dir = base_dir / "archive" / "youtube"
    raw_dir.mkdir(parents=True, exist_ok=True)
    archive_dir.mkdir(parents=True, exist_ok=True)
    
    # 9. ä¿å­˜åŸå§‹æ•°æ®
    raw_file = raw_dir / f"{today}_raw.json"
    raw_data = {
        "date": today,
        "themes_used": themes,
        "whitelist_channels": whitelist_channels,
        "days_window": DAYS_WINDOW,
        "total_videos_found": len(all_videos),
        "unique_videos": len(unique_videos),
        "filtered_videos_count": len(filtered_videos),
        "all_videos": sorted_videos  # ä¿å­˜è¿‡æ»¤åçš„è§†é¢‘ï¼ŒæŒ‰è¯„åˆ†æ’åº
    }
    
    try:
        with open(raw_file, 'w', encoding='utf-8') as f:
            json.dump(raw_data, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜åˆ°: {raw_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜åŸå§‹æ•°æ®å¤±è´¥: {e}")
        return
    
    # 10. ç”Ÿæˆå¹¶ä¿å­˜æ—¥æŠ¥
    action = generate_action(top3_videos)
    risk = generate_risk(sorted_videos, themes)
    
    daily_report = {
        "date": today,
        "themes_used": themes,
        "dimensions": [],  # æ–°å¢ï¼šç”¨æˆ·å¯é€‰çš„ç»´åº¦æ ‡ç­¾ï¼ˆå¦‚ï¼š["å¥åº·", "æƒ…ç»ª", "å·¥ä½œ"]ï¼‰ï¼Œå‘åå…¼å®¹
        "top3": [
            {
                "title": video['title'],
                "channel": video['channel_title'],
                "url": video['url'],
                "score": video['score'],
                "published_at": video['published_at'],
                "scoring_details": video['scoring_details']
            }
            for video in top3_videos
        ],
        "action": action,
        "risk": risk
    }
    
    archive_file = archive_dir / f"{today}.json"
    try:
        with open(archive_file, 'w', encoding='utf-8') as f:
            json.dump(daily_report, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ æ—¥æŠ¥ä¿¡å·å·²ä¿å­˜åˆ°: {archive_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ—¥æŠ¥ä¿¡å·å¤±è´¥: {e}")
        return
    
    # 11. å¦‚æœæ¨¡å¼æ˜¯ researchï¼Œç”Ÿæˆç ”ç©¶æŠ¥å‘Š
    if mode == "research":
        llm = init_research_llm()
        if llm:
            try:
                research_report = generate_research_report(top3_videos, themes, llm)
                if research_report:
                    # æ·»åŠ  sources å­—æ®µ
                    research_report["sources"] = prepare_sources_data(top3_videos)
                    research_report["date"] = today
                    research_report["themes_used"] = themes
                    
                    # ä¿å­˜ç ”ç©¶æŠ¥å‘Š
                    research_file = archive_dir / f"{today}_research.json"
                    with open(research_file, 'w', encoding='utf-8') as f:
                        json.dump(research_report, f, indent=2, ensure_ascii=False)
                    print(f"\nğŸ’¾ ç ”ç©¶æŠ¥å‘Šå·²ä¿å­˜åˆ°: {research_file}")
                    
                    # æ˜¾ç¤ºç ”ç©¶æŠ¥å‘Šæ‘˜è¦
                    print("\n" + "=" * 70)
                    print("ğŸ”¬ ç ”ç©¶æŠ¥å‘Šæ‘˜è¦")
                    print("=" * 70)
                    print(f"æ ¸å¿ƒé—®é¢˜: {research_report.get('question', 'N/A')}")
                    print(f"\nå…³é”®å‘ç°:")
                    for i, finding in enumerate(research_report.get('key_findings', []), 1):
                        print(f"  {i}. {finding}")
                    print(f"\nä¸ºä»€ä¹ˆé‡è¦: {research_report.get('why_it_matters_to_me', 'N/A')}")
                    print(f"\nä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
                    for i, step in enumerate(research_report.get('next_steps', []), 1):
                        print(f"  {i}. {step}")
                    print("=" * 70)
                else:
                    print("âš ï¸  ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œå·²è·³è¿‡")
            except Exception as e:
                print(f"âš ï¸  ç”Ÿæˆç ”ç©¶æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
        else:
            print("âš ï¸  æœªé…ç½® LLMï¼Œè·³è¿‡ç ”ç©¶æ¨¡å¼")
    
    # 12. æ˜¾ç¤ºæ—¥æŠ¥æ‘˜è¦
    print("\n" + "=" * 70)
    print("ğŸ“„ æ—¥æŠ¥æ‘˜è¦")
    print("=" * 70)
    print(f"æ—¥æœŸ: {daily_report['date']}")
    print(f"ä¸»é¢˜: {', '.join(daily_report['themes_used'])}")
    print(f"\næ¨èè¡ŒåŠ¨ (Action):")
    print(f"  {daily_report['action']}")
    print(f"\né£é™©è¯„ä¼° (Risk):")
    print(f"  {daily_report['risk']}")
    print("=" * 70)


if __name__ == "__main__":
    main()
