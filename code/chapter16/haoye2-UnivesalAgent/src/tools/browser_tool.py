import requests
from bs4 import BeautifulSoup
from urllib.parse import quote_plus, urljoin
import time
import re
import html

class BrowserTool:
    name = "browser_search"
    description = "æ‰§è¡Œç½‘é¡µæœç´¢ï¼ˆæ”¯æŒå¤šç§æœç´¢å¼•æ“å’Œå†…å®¹æå–ï¼‰"
    
    def get_parameters(self):
        return {
            "input": {"type": "str", "description": "æœç´¢å…³é”®è¯", "required": True}
        }

    def _is_valid_result(self, title, url):
        """éªŒè¯æœç´¢ç»“æœçš„æœ‰æ•ˆæ€§"""
        if not title or len(title.strip()) < 3:
            return False
        
        # è¿‡æ»¤å¯¼èˆªé“¾æ¥å’Œæ— æ„ä¹‰å†…å®¹
        skip_keywords = [
            "next", "previous", "more", "about", "help", "settings",
            "privacy", "terms", "feedback", "donate", "install",
            "download", "login", "register", "sign in", "sign up"
        ]
        
        title_lower = title.lower()
        if any(keyword in title_lower for keyword in skip_keywords):
            return False
        
        # è¿‡æ»¤å¹¿å‘Šå’Œæ¨å¹¿é“¾æ¥
        ad_indicators = ["ad", "sponsored", "promotion", "å¹¿å‘Š", "æ¨å¹¿"]
        if any(indicator in title_lower for indicator in ad_indicators):
            return False
        
        return True

    def _clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.strip())
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()[\]{}"\'-]', '', text)
        
        return text[:200]  # é™åˆ¶é•¿åº¦

    def _search_searx(self, query, limit=5):
        """ä½¿ç”¨å¤šä¸ªæœç´¢å¼•æ“å®ä¾‹ - ç¨³å®šç‰ˆï¼Œä¼˜å…ˆæ”¯æŒä¸­æ–‡æœç´¢"""
        # ç²¾é€‰å¤šä¸ªç¨³å®šçš„æœç´¢å¼•æ“ï¼Œä¼˜å…ˆæ”¯æŒä¸­æ–‡
        search_instances = [
            {
                "name": "Searx.xyz",
                "url": "https://searx.xyz/search",
                "timeout": 10,
                "type": "searx"
            },
            {
                "name": "Searx.be",
                "url": "https://searx.be/search",
                "timeout": 10,
                "type": "searx"
            },
            {
                "name": "Braveæœç´¢",
                "url": "https://search.brave.com/search",
                "timeout": 8,
                "type": "brave"
            },
            {
                "name": "Ecosia",
                "url": "https://www.ecosia.org/search",
                "timeout": 8,
                "type": "ecosia"
            },
            {
                "name": "Qwant",
                "url": "https://www.qwant.com",
                "timeout": 8,
                "type": "qwant"
            }
        ]
        
        for instance in search_instances:
            try:
                print(f"ğŸ” å°è¯• {instance['name']}...")
                result = self._try_search_instance(instance, query, limit)
                if result and len(result) > 0:
                    print(f"âœ… {instance['name']} æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(result)} ä¸ªç»“æœ")
                    return result, True
                    
            except Exception as e:
                print(f"âš ï¸ {instance['name']} å¤±è´¥: {str(e)[:50]}")
                continue  # é™é»˜å¤±è´¥ï¼Œå¿«é€Ÿåˆ‡æ¢
        
        # å¿«é€Ÿé™çº§åˆ°æœç´¢å»ºè®®
        print("ğŸ”— æ‰€æœ‰æœç´¢å¼•æ“å¤±è´¥ï¼Œæä¾›æœç´¢å»ºè®®")
        return self._get_search_suggestions(query), True

    def _try_search_instance(self, instance, query, limit):
        """å°è¯•å•ä¸ªæœç´¢å¼•æ“å®ä¾‹"""
        if instance['type'] == 'searx':
            return self._try_searx_instance(instance, query, limit)
        elif instance['type'] == 'duckduckgo':
            return self._try_duckduckgo_instance(instance, query, limit)
        elif instance['type'] == 'startpage':
            return self._try_startpage_instance(instance, query, limit)
        elif instance['type'] == 'qwant':
            return self._try_qwant_instance(instance, query, limit)
        elif instance['type'] == 'brave':
            return self._try_brave_instance(instance, query, limit)
        elif instance['type'] == 'ecosia':
            return self._try_ecosia_instance(instance, query, limit)
        else:
            return None

    def _try_searx_instance(self, instance, query, limit):
        """å°è¯•Searxå®ä¾‹ - ä¼˜åŒ–ä¸­æ–‡æœç´¢æ”¯æŒ"""
        # æ£€æµ‹æ˜¯å¦ä¸ºä¸­æ–‡æŸ¥è¯¢
        is_chinese = any('\u4e00' <= char <= '\u9fff' for char in query)
        
        params = {
            'q': query,
            'format': 'json',
            'engines': 'google,bing,duckduckgo,yandex' if not is_chinese else 'google,bing,yandex,baidu',
            'language': 'zh-CN' if is_chinese else 'auto'
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8" if is_chinese else "en-US,en;q=0.9"
        }
        
        try:
            response = requests.get(
                instance['url'], 
                params=params, 
                headers=headers, 
                timeout=instance['timeout']
            )
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    results = []
                    
                    for item in data.get('results', [])[:limit]:
                        title = self._clean_text(item.get('title', ''))
                        url = item.get('url', '')
                        content = item.get('content', '')
                        
                        if self._is_valid_result(title, url):
                            results.append({
                                'title': title,
                                'url': url,
                                'snippet': self._clean_text(content)[:200],
                                'source': f"{instance['name']}/{item.get('engine', 'unknown')}"
                            })
                    
                    return results if results else None
                except Exception as e:
                    print(f"âš ï¸ è§£æSearxå“åº”å¤±è´¥: {str(e)[:50]}")
                    return None
            else:
                print(f"âš ï¸ Searxè¿”å›çŠ¶æ€ç : {response.status_code}")
                return None
        except requests.Timeout:
            print(f"âš ï¸ {instance['name']} è¯·æ±‚è¶…æ—¶")
            return None
        except Exception as e:
            print(f"âš ï¸ {instance['name']} è¯·æ±‚å¼‚å¸¸: {str(e)[:50]}")
            return None

    def _try_duckduckgo_instance(self, instance, query, limit):
        """å°è¯•DuckDuckGoå®ä¾‹"""
        params = {
            'q': query,
            'kl': 'cn-zh'
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }
        
        response = requests.get(
            instance['url'], 
            params=params, 
            headers=headers, 
            timeout=instance['timeout']
        )
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            return self._extract_duckduckgo_results_from_soup(soup, limit)
        
        return None

    def _try_startpage_instance(self, instance, query, limit):
        """å°è¯•Startpageå®ä¾‹"""
        params = {
            'query': query,
            'cat': 'web',
            'pl': 'ext-ff',
            'extVersion': '1.3.0'
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }
        
        response = requests.get(
            instance['url'], 
            params=params, 
            headers=headers, 
            timeout=instance['timeout']
        )
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            return self._extract_startpage_results(soup, limit)
        
        return None

    def _try_qwant_instance(self, instance, query, limit):
        """å°è¯•Qwantå®ä¾‹"""
        params = {
            'q': query,
            't': 'web',
            'locale': 'zh_CN'
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }
        
        response = requests.get(
            instance['url'], 
            params=params, 
            headers=headers, 
            timeout=instance['timeout']
        )
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            return self._extract_qwant_results(soup, limit)
        
        return None

    def _try_brave_instance(self, instance, query, limit):
        """å°è¯•Braveæœç´¢å®ä¾‹"""
        params = {
            'q': query,
            'source': 'web'
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
        }
        
        try:
            response = requests.get(
                instance['url'], 
                params=params, 
                headers=headers, 
                timeout=instance['timeout']
            )
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                # Braveæœç´¢ç»“æœæå–ï¼ˆéœ€è¦æ ¹æ®å®é™…HTMLç»“æ„è°ƒæ•´ï¼‰
                results = []
                result_divs = soup.find_all('div', class_=['result', 'web-result'])
                
                for div in result_divs[:limit]:
                    title_elem = div.find('a') or div.find('h2')
                    snippet_elem = div.find('p') or div.find('span', class_='snippet')
                    
                    if title_elem:
                        title = self._clean_text(title_elem.get_text())
                        url = title_elem.get('href', '')
                        snippet = self._clean_text(snippet_elem.get_text()) if snippet_elem else ''
                        
                        if self._is_valid_result(title, url):
                            results.append({
                                'title': title,
                                'url': url,
                                'snippet': snippet[:200],
                                'source': 'Brave'
                            })
                
                return results if results else None
        except Exception:
            return None
        
        return None

    def _try_ecosia_instance(self, instance, query, limit):
        """å°è¯•Ecosiaæœç´¢å®ä¾‹"""
        params = {
            'q': query
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
        }
        
        try:
            response = requests.get(
                instance['url'], 
                params=params, 
                headers=headers, 
                timeout=instance['timeout']
            )
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                # Ecosiaæœç´¢ç»“æœæå–ï¼ˆéœ€è¦æ ¹æ®å®é™…HTMLç»“æ„è°ƒæ•´ï¼‰
                results = []
                result_divs = soup.find_all('div', class_=['result', 'web-result', 'result__body'])
                
                for div in result_divs[:limit]:
                    title_elem = div.find('a') or div.find('h2')
                    snippet_elem = div.find('p') or div.find('span', class_='result__snippet')
                    
                    if title_elem:
                        title = self._clean_text(title_elem.get_text())
                        url = title_elem.get('href', '')
                        snippet = self._clean_text(snippet_elem.get_text()) if snippet_elem else ''
                        
                        if self._is_valid_result(title, url):
                            results.append({
                                'title': title,
                                'url': url,
                                'snippet': snippet[:200],
                                'source': 'Ecosia'
                            })
                
                return results if results else None
        except Exception:
            return None
        
        return None

    def _extract_duckduckgo_results_from_soup(self, soup, limit):
        """ä»DuckDuckGo HTMLä¸­æå–ç»“æœ"""
        results = []
        
        # æŸ¥æ‰¾æœç´¢ç»“æœ
        result_divs = soup.find_all('div', class_='result')
        
        for div in result_divs[:limit]:
            title_elem = div.find('a', class_='result__a')
            snippet_elem = div.find('a', class_='result__snippet')
            
            if title_elem:
                title = self._clean_text(title_elem.get_text())
                url = title_elem.get('href', '')
                snippet = self._clean_text(snippet_elem.get_text()) if snippet_elem else ''
                
                if self._is_valid_result(title, url):
                    results.append({
                        'title': title,
                        'url': url,
                        'snippet': snippet[:200],
                        'source': 'DuckDuckGo'
                    })
        
        return results

    def _extract_startpage_results(self, soup, limit):
        """ä»Startpage HTMLä¸­æå–ç»“æœ"""
        results = []
        
        # æŸ¥æ‰¾æœç´¢ç»“æœ
        result_divs = soup.find_all('div', class_='w-gl__result')
        
        for div in result_divs[:limit]:
            title_elem = div.find('h3')
            link_elem = title_elem.find('a') if title_elem else None
            snippet_elem = div.find('p', class_='w-gl__description')
            
            if link_elem:
                title = self._clean_text(link_elem.get_text())
                url = link_elem.get('href', '')
                snippet = self._clean_text(snippet_elem.get_text()) if snippet_elem else ''
                
                if self._is_valid_result(title, url):
                    results.append({
                        'title': title,
                        'url': url,
                        'snippet': snippet[:200],
                        'source': 'Startpage'
                    })
        
        return results

    def _extract_qwant_results(self, soup, limit):
        """ä»Qwant HTMLä¸­æå–ç»“æœ"""
        results = []
        
        # æŸ¥æ‰¾æœç´¢ç»“æœ
        result_divs = soup.find_all('div', class_='result')
        
        for div in result_divs[:limit]:
            title_elem = div.find('a', class_='result--web')
            snippet_elem = div.find('p', class_='result__desc')
            
            if title_elem:
                title = self._clean_text(title_elem.get_text())
                url = title_elem.get('href', '')
                snippet = self._clean_text(snippet_elem.get_text()) if snippet_elem else ''
                
                if self._is_valid_result(title, url):
                    results.append({
                        'title': title,
                        'url': url,
                        'snippet': snippet[:200],
                        'source': 'Qwant'
                    })
        
        return results

    def _extract_duckduckgo_results(self, soup, limit=5):
        """æå–DuckDuckGoæœç´¢ç»“æœ"""
        results = []
        
        # DuckDuckGoç°åœ¨è¿”å›202çŠ¶æ€ç ï¼Œéœ€è¦JavaScriptæ¸²æŸ“
        # æˆ‘ä»¬å°è¯•ä»HTMLä¸­æå–ä»»ä½•æœ‰ç”¨çš„ä¿¡æ¯
        
        # æ–¹æ³•1ï¼šæŸ¥æ‰¾æ‰€æœ‰å¤–éƒ¨é“¾æ¥
        all_links = soup.find_all('a', href=True)
        external_links = []
        
        for link in all_links:
            href = link.get('href', '')
            title = self._clean_text(link.get_text(strip=True))
            
            # è¿‡æ»¤å¤–éƒ¨é“¾æ¥ï¼ˆéDuckDuckGoå†…éƒ¨é“¾æ¥ï¼‰
            if (href and 
                not href.startswith('javascript:') and
                not href.startswith('#') and
                'duckduckgo.com' not in href and
                len(title) > 3 and
                self._is_valid_result(title, href)):
                
                external_links.append({
                    'title': title,
                    'url': href,
                    'snippet': '',
                    'link_element': link
                })
        
        # æ–¹æ³•2ï¼šå¦‚æœå¤–éƒ¨é“¾æ¥ä¸å¤Ÿï¼Œå°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–ä¿¡æ¯
        if len(external_links) < 2:
            print("âš ï¸ å¤–éƒ¨é“¾æ¥è¾ƒå°‘ï¼Œå°è¯•æ–‡æœ¬æå–")
            
            # æŸ¥æ‰¾é¡µé¢ä¸­çš„ä¸»è¦æ–‡æœ¬å†…å®¹
            text_content = soup.get_text()
            
            # å°è¯•æå–URLæ¨¡å¼
            import re
            url_pattern = r'https?://[^\s<>"\'()]+'
            urls = re.findall(url_pattern, text_content)
            
            for url in urls[:limit]:
                # ä»URLä¸­æå–å¯èƒ½çš„æ ‡é¢˜
                domain = url.split('/')[2] if '/' in url else url
                title = domain.replace('www.', '').title()
                
                if self._is_valid_result(title, url):
                    external_links.append({
                        'title': title,
                        'url': url,
                        'snippet': f'æ¥è‡ª {domain}',
                        'link_element': None
                    })
        
        # æ–¹æ³•3ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰è¶³å¤Ÿç»“æœï¼Œæä¾›æœç´¢å»ºè®®
        if len(external_links) < 2:
            print("âš ï¸ æœç´¢ç»“æœæœ‰é™ï¼Œæä¾›æœç´¢å»ºè®®")
            
            suggestions = [
                {
                    'title': f'åœ¨Googleæœç´¢ "{self.last_query}"',
                    'url': f'https://www.google.com/search?q={self.last_query}',
                    'snippet': 'ä½¿ç”¨Googleæœç´¢å¼•æ“',
                    'link_element': None
                },
                {
                    'title': f'åœ¨Bingæœç´¢ "{self.last_query}"',
                    'url': f'https://www.bing.com/search?q={self.last_query}',
                    'snippet': 'ä½¿ç”¨Bingæœç´¢å¼•æ“',
                    'link_element': None
                }
            ]
            external_links.extend(suggestions)
        
        # å»é‡å¹¶é™åˆ¶ç»“æœæ•°é‡
        seen_urls = set()
        unique_results = []
        
        for result in external_links:
            if result['url'] and result['url'] not in seen_urls:
                seen_urls.add(result['url'])
                unique_results.append(result)
                if len(unique_results) >= limit:
                    break
        
        return unique_results

    def _extract_content_from_url(self, url, max_length=300):
        """ä»URLæå–ä¸»è¦å†…å®¹"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                return "å†…å®¹è·å–å¤±è´¥"
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for script in soup(["script", "style", "nav", "footer", "header", "aside", "advertisement"]):
                script.decompose()
            
            # æ™ºèƒ½å†…å®¹æå–ç­–ç•¥
            content = self._extract_main_content(soup)
            
            if not content:
                content = soup.get_text(strip=True)
            
            # æ¸…ç†å’Œä¼˜åŒ–å†…å®¹
            content = self._clean_and_format_content(content)
            
            return content[:max_length] + "..." if len(content) > max_length else content
            
        except Exception as e:
            return f"å†…å®¹æå–å¤±è´¥: {str(e)[:50]}"

    def _extract_main_content(self, soup):
        """æ™ºèƒ½æå–é¡µé¢ä¸»è¦å†…å®¹"""
        # ä¼˜å…ˆçº§ç­–ç•¥ï¼šä»æœ€å…·ä½“åˆ°æœ€é€šç”¨
        extraction_strategies = [
            # 1. æ–‡ç« ç›¸å…³æ ‡ç­¾
            ['article', 'main article', '.article-content', '.post-content'],
            # 2. ä¸»è¦å†…å®¹åŒºåŸŸ
            ['main', '.main', '.content', '.main-content'],
            # 3. å¸¸è§å†…å®¹ç±»å
            ['.entry-content', '.post-body', '.article-body', '.content-area'],
            # 4. é€šç”¨å®¹å™¨
            ['.container', '.wrapper', '.page-content'],
            # 5. æœ€åå°è¯•body
            ['body']
        ]
        
        for strategy in extraction_strategies:
            for selector in strategy:
                element = soup.select_one(selector)
                if element:
                    content = element.get_text(strip=True)
                    # éªŒè¯å†…å®¹è´¨é‡
                    if self._is_quality_content(content):
                        return content
        
        return ""

    def _is_quality_content(self, content):
        """éªŒè¯å†…å®¹è´¨é‡"""
        if not content or len(content) < 50:
            return False
        
        # è¿‡æ»¤å¯¼èˆªå’Œèœå•å†…å®¹
        nav_keywords = ['å¯¼èˆª', 'èœå•', 'é¦–é¡µ', 'ç™»å½•', 'æ³¨å†Œ', 'æœç´¢', 'è”ç³»', 'å…³äº', 'privacy', 'terms', 'home', 'login', 'register', 'contact', 'about']
        content_lower = content.lower()
        
        for keyword in nav_keywords:
            if keyword in content_lower:
                return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„å¥å­
        sentences = content.split('ã€‚')
        meaningful_sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        return len(meaningful_sentences) >= 2

    def _clean_and_format_content(self, content):
        """æ¸…ç†å’Œæ ¼å¼åŒ–å†…å®¹"""
        if not content:
            return ""
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r'\s+', ' ', content.strip())
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡æ ‡ç‚¹
        content = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()[\]{}"\'ã€‚ï¼Œï¼ï¼Ÿï¼šï¼›ï¼ˆï¼‰ã€ã€‘""''-]', '', content)
        
        # ç§»é™¤é‡å¤çš„æ¢è¡Œå’Œç©ºæ ¼
        content = re.sub(r'\n\s*\n', '\n', content)
        content = re.sub(r' {2,}', ' ', content)
        
        # æå–å‰å‡ ä¸ªæœ‰æ„ä¹‰çš„å¥å­
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
        meaningful_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10 and len(sentence) < 100:  # åˆç†çš„å¥å­é•¿åº¦
                meaningful_sentences.append(sentence)
                if len(meaningful_sentences) >= 3:  # æœ€å¤š3ä¸ªå¥å­
                    break
        
        return 'ã€‚'.join(meaningful_sentences)

    def _enhance_search_results(self, results, limit=3):
        """å¢å¼ºæœç´¢ç»“æœï¼Œæå–å†…å®¹é¢„è§ˆ"""
        enhanced_results = []
        
        for i, result in enumerate(results):
            if i >= limit:  # åªå¢å¼ºå‰å‡ ä¸ªç»“æœ
                break
            
            if result['url'] and result['url'].startswith('http'):
                print(f"ğŸ“„ æå–å†…å®¹: {result['title'][:30]}...")
                content = self._extract_content_from_url(result['url'])
                result['snippet'] = content
                result['enhanced'] = True
            else:
                result['enhanced'] = False
            
            enhanced_results.append(result)
        
        # æ·»åŠ æœªå¢å¼ºçš„ç»“æœ
        enhanced_results.extend(results[limit:])
        
        return enhanced_results

    def _fallback_extraction(self, soup, limit=5):
        """å¤‡ç”¨ç»“æœæå–æ–¹æ³•"""
        results = []
        
        # æ–¹æ³•1ï¼šæå–æ ‡é¢˜å…ƒç´ 
        for tag in ["h1", "h2", "h3", "h4"]:
            elements = soup.find_all(tag)
            for elem in elements:
                if len(results) >= limit:
                    break
                    
                title = self._clean_text(elem.get_text(strip=True))
                if self._is_valid_result(title, ""):
                    results.append({
                        "title": title,
                        "url": "",
                        "snippet": ""
                    })
        
        # æ–¹æ³•2ï¼šæå–æ–‡æœ¬å—
        if not results:
            text_blocks = soup.get_text().split('\n')
            for block in text_blocks:
                if len(results) >= limit:
                    break
                    
                block = self._clean_text(block)
                if len(block) > 20 and len(block) < 150:
                    results.append({
                        "title": block,
                        "url": "",
                        "snippet": ""
                    })
        
        return results

    def run(self, parameters):
        # ç¡®ä¿å‚æ•°å¤„ç†çš„å®‰å…¨æ€§
        if isinstance(parameters, dict):
            query = parameters.get("input", "")
        else:
            query = str(parameters) if parameters else ""

        # å‚æ•°éªŒè¯
        if not query or not query.strip():
            return "é”™è¯¯ï¼šæœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º"
        
        query = query.strip()
        self.last_query = query  # ä¿å­˜æŸ¥è¯¢ç”¨äºå»ºè®®
        limit = 5  # å¢åŠ ç»“æœæ•°é‡
        
        # URL ç¼–ç æŸ¥è¯¢å‚æ•°
        encoded_query = quote_plus(query)
        url = f"https://duckduckgo.com/html/?q={encoded_query}"
        
        # ä½¿ç”¨æ›´çœŸå®çš„User-Agent
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
        # æ£€æµ‹æ˜¯å¦ä¸ºä¸­æ–‡æŸ¥è¯¢
        is_chinese = any('\u4e00' <= char <= '\u9fff' for char in query)
        
        # å¯¹äºä¸­æ–‡æœç´¢ï¼Œç›´æ¥ä½¿ç”¨Searxæœç´¢å¼•æ“ï¼Œè·³è¿‡DuckDuckGoï¼ˆé¿å…202é—®é¢˜ï¼‰
        if is_chinese:
            print(f"ğŸŒ æ£€æµ‹åˆ°ä¸­æ–‡æŸ¥è¯¢ï¼Œä½¿ç”¨å¤šå¼•æ“æœç´¢ç­–ç•¥...")
            searx_results, searx_success = self._search_searx(query, limit)
            
            if searx_success and searx_results:
                results = searx_results
                search_engine = "Searxå¤šå¼•æ“"
                print(f"âœ… ä¸­æ–‡æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            else:
                # å¦‚æœSearxå¤±è´¥ï¼Œæä¾›æœç´¢å»ºè®®
                print("âš ï¸ æ‰€æœ‰æœç´¢å¼•æ“å¤±è´¥ï¼Œæä¾›æœç´¢å»ºè®®")
                results = self._get_search_suggestions(query)
                search_engine = "æœç´¢å»ºè®®"
        else:
            # è‹±æ–‡æœç´¢ï¼šå…ˆå°è¯•DuckDuckGoï¼Œå¤±è´¥åä½¿ç”¨Searx
            max_retries = 2  # å‡å°‘é‡è¯•æ¬¡æ•°ï¼Œå¿«é€Ÿåˆ‡æ¢åˆ°Searx
            duckduckgo_success = False
            
            for attempt in range(max_retries):
                try:
                    print(f"ğŸ” å°è¯•DuckDuckGoæœç´¢: {query} (å°è¯• {attempt + 1}/{max_retries})")
                    
                    response = requests.get(url, headers=headers, timeout=10)
                    
                    # DuckDuckGoç»å¸¸è¿”å›202ï¼Œç›´æ¥è·³è¿‡
                    if response.status_code == 202:
                        print("âš ï¸ DuckDuckGoè¿”å›202ï¼ˆéœ€è¦JavaScriptï¼‰ï¼Œåˆ‡æ¢åˆ°Searx...")
                        break
                    
                    if response.status_code != 200:
                        if attempt < max_retries - 1:
                            time.sleep(1)
                            continue
                        break
                    
                    # æ£€æŸ¥å“åº”å†…å®¹
                    if len(response.text) < 1000:
                        if attempt < max_retries - 1:
                            time.sleep(1)
                            continue
                        break
                    
                    soup = BeautifulSoup(response.text, "html.parser")
                    results = self._extract_duckduckgo_results(soup, limit)
                    
                    if results and len(results) > 0:
                        duckduckgo_success = True
                        search_engine = "DuckDuckGo"
                        print(f"âœ… DuckDuckGoæœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                        break
                        
                except Exception as e:
                    print(f"âš ï¸ DuckDuckGoå°è¯•å¤±è´¥: {str(e)[:50]}")
                    if attempt < max_retries - 1:
                        time.sleep(1)
                        continue
                    break
            
            # å¦‚æœDuckDuckGoå¤±è´¥ï¼Œä½¿ç”¨Searx
            if not duckduckgo_success:
                print("ğŸŒ DuckDuckGoå¤±è´¥ï¼Œåˆ‡æ¢åˆ°Searxæœç´¢å¼•æ“...")
                searx_results, searx_success = self._search_searx(query, limit)
                
                if searx_success and searx_results:
                    results = searx_results
                    search_engine = "Searxå¤šå¼•æ“"
                    print(f"âœ… Searxæœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                else:
                    print("âš ï¸ æ‰€æœ‰æœç´¢å¼•æ“å¤±è´¥ï¼Œæä¾›æœç´¢å»ºè®®")
                    results = self._get_search_suggestions(query)
                    search_engine = "æœç´¢å»ºè®®"
        
        # å¢å¼ºæœç´¢ç»“æœï¼ˆæå–å†…å®¹é¢„è§ˆï¼‰
        if results:
            print("ğŸš€ å¢å¼ºæœç´¢ç»“æœï¼Œæå–å†…å®¹é¢„è§ˆ...")
            enhanced_results = self._enhance_search_results(results, limit=3)
            results = enhanced_results
        
        # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
        if results:
            formatted_results = []
            for i, result in enumerate(results, 1):
                result_text = f"{i}. {result['title']}"
                
                if result['url']:
                    result_text += f"\n   ğŸ”— {result['url']}"
                
                if result['snippet']:
                    # å¦‚æœæ˜¯å¢å¼ºçš„ç»“æœï¼Œæ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                    if result.get('enhanced'):
                        result_text += f"\n   ğŸ“„ å†…å®¹é¢„è§ˆ: {result['snippet']}"
                    else:
                        result_text += f"\n   ğŸ“ {result['snippet']}"
                
                formatted_results.append(result_text)
            
            return "\n\n".join(formatted_results)
        else:
            return f"æœªæ‰¾åˆ°å…³äº '{query}' çš„æœç´¢ç»“æœã€‚è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯ã€‚"
        
    def _get_search_suggestions(self, query):
        """å¿«é€Ÿæä¾›æœç´¢å»ºè®®"""
        return [
            {
                'title': f'Googleæœç´¢: {query}',
                'url': f'https://www.google.com/search?q={query}',
                'snippet': 'ä½¿ç”¨Googleæœç´¢å¼•æ“',
                'source': 'Google'
            },
            {
                'title': f'Bingæœç´¢: {query}',
                'url': f'https://www.bing.com/search?q={query}',
                'snippet': 'ä½¿ç”¨Bingæœç´¢å¼•æ“',
                'source': 'Bing'
            }
        ]

        return "æœç´¢å¤±è´¥ï¼Œå·²å¤šæ¬¡é‡è¯•ã€‚è¯·ç¨åå†è¯•ã€‚"
